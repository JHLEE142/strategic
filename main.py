import io
import json
import math
import os
import re
from collections import Counter
from datetime import datetime
from functools import lru_cache
from typing import Dict, List, Optional, Tuple

import pandas as pd
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI

KEYWORD_RULES = [
    {"tag": "ì„í”Œë€íŠ¸ ê´€ì‹¬", "keywords": ["ì„í”Œë€íŠ¸", "implant"], "weight": 25},
    {"tag": "ì¥ë¹„ ë°ëª¨ ìš”ì²­", "keywords": ["ë°ëª¨", "ì‹œì—°", "ì²´í—˜"], "weight": 20},
    {"tag": "ì˜ˆì‚° ê²€í†  ì¤‘", "keywords": ["ì˜ˆì‚°", "ê²¬ì ", "ë¹„ìš©"], "weight": 15},
    {"tag": "ì¬ë°©ë¬¸ ìš”ì²­", "keywords": ["ì¬ë°©ë¬¸", "ë‹¤ìŒ ë°©ë¬¸", "ì¶”ê°€ ë°©ë¬¸"], "weight": 20},
    {"tag": "ê³„ì•½ ì„ë°•", "keywords": ["ê³„ì•½", "í™•ì •", "ì§„í–‰"], "weight": 30},
]

POSITIVE_PATTERNS = ["ê¸ì •", "í˜¸ì˜", "ë„ì…", "ì¬ë°©ë¬¸", "ì¶”ì§„", "ê´€ì‹¬"]
NEUTRAL_PATTERNS = ["ê²€í† ", "ë³´ë¥˜", "ìˆ™ê³ ", "í™•ì¸"]
NEGATIVE_PATTERNS = ["ê±°ì ˆ", "ì·¨ì†Œ", "ë¯¸ì •", "ì—°ê¸°", "ì–´ë ¤ì›€"]

SENTIMENT_WEIGHTS = {"positive": 20, "neutral": 5, "negative": -20}

HOT_THRESHOLD = 70
WARM_THRESHOLD = 40
MAX_RESULTS = 300
AI_MODEL = "Qwen/Qwen3-4B-Instruct-2507:nscale"
AI_BASE_URL = "https://router.huggingface.co/v1"


load_dotenv()


@lru_cache(maxsize=1)
def get_openai_client() -> Optional[OpenAI]:
    api_key = os.getenv("HUGGINGFACE_API_KEY")
    if not api_key:
        return None
    return OpenAI(api_key=api_key, base_url=AI_BASE_URL)


def build_ai_prompt(user_prompt: str, df: pd.DataFrame, tokens: List[str]) -> str:
    preview_records = df.head(20).to_dict(orient="records")
    serialized_preview = json.dumps(preview_records, ensure_ascii=False, indent=2)
    prompt_tokens = ", ".join(tokens) if tokens else "í‚¤ì›Œë“œ ì—†ìŒ"

    return (
        "ë‹¹ì‹ ì€ B2B ê±°ë˜ì²˜ë¥¼ ê´€ë¦¬í•˜ëŠ” ê³ ê¸‰ ì˜ì—… ì „ëµê°€ì…ë‹ˆë‹¤. "
        "ë‹¤ìŒ ë°ì´í„°ëŠ” í•œêµ­ì–´ë¡œ ì •ë¦¬ëœ ê±°ë˜ì²˜ ìƒë‹´ ê¸°ë¡ì…ë‹ˆë‹¤. "
        "ëª©í‘œëŠ” ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë°”íƒ•ìœ¼ë¡œ ì „ëµì  ì œì•ˆì„ êµ¬ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n\n"
        "ì‘ë‹µ ì‹œ ë‹¤ìŒ ìš”ì†Œë¥¼ í¬í•¨í•˜ì„¸ìš”:\n"
        "1. í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ìš”ì•½ (3ì¤„ ì´ë‚´)\n"
        "2. ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ê±°ë˜ì²˜ ì•¡ì…˜ í”Œëœ (Hot/Warm/Cold ê¸°ì¤€ìœ¼ë¡œ ìµœëŒ€ 5ê°œ)\n"
        "3. ì¶”ê°€ì ìœ¼ë¡œ ì œì•ˆí•  ì˜ì—… ì „ëµ ì•„ì´ë””ì–´\n"
        "4. í›„ì† ê´€ë¦¬ ì²´í¬ë¦¬ìŠ¤íŠ¸ (ê°„ë‹¨í•œ bullet)\n\n"
        f"- ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸: {user_prompt or 'ì…ë ¥ ì—†ìŒ'}\n"
        f"- ì¶”ì¶œëœ í‚¤ì›Œë“œ: {prompt_tokens}\n"
        "- ì°¸ê³  ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ìµœëŒ€ 20ê±´):\n"
        f"{serialized_preview}\n\n"
        "í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ë©´ì„œë„ ì‹¤í–‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì œì•ˆí•˜ì„¸ìš”."
    )


def generate_ai_strategy(user_prompt: str, df: pd.DataFrame, tokens: List[str]) -> str:
    client = get_openai_client()
    if client is None:
        return "âš ï¸ `HUGGINGFACE_API_KEY` í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ AI ì „ëµì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    if df.empty:
        return "í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ì–´ AI ì „ëµì„ ìƒì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    composed_prompt = build_ai_prompt(user_prompt, df, tokens)

    try:
        response = client.chat.completions.create(
            model=AI_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ìƒë‹´ ì „ëµì„ ì„¤ê³„í•˜ëŠ” ì˜ì—… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
                },
                {"role": "user", "content": composed_prompt},
            ],
            temperature=0.4,
            max_tokens=600,
        )
    except Exception as error:  # pylint: disable=broad-except
        return f"AI ì „ëµ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error}"

    choices = getattr(response, "choices", [])
    if not choices:
        return "AI ëª¨ë¸ë¡œë¶€í„° ìœ íš¨í•œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    message = choices[0].message
    return getattr(message, "content", "AI ëª¨ë¸ ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")


def tokenize_prompt(prompt: str) -> List[str]:
    return [token for token in re.split(r"\s+", prompt.strip()) if token]


def normalize_text(value: object) -> str:
    if pd.isna(value):
        return ""
    return str(value).strip().lower()


def combine_row_text(row: pd.Series) -> str:
    return " ".join(normalize_text(value) for value in row if str(value).strip())


def keyword_features(text: str) -> Tuple[List[str], int]:
    tags: List[str] = []
    score = 0
    for rule in KEYWORD_RULES:
        if any(keyword in text for keyword in rule["keywords"]):
            tags.append(rule["tag"])
            score += rule["weight"]
    return tags, score


def detect_sentiment(text: str) -> Tuple[str, int]:
    if any(token in text for token in POSITIVE_PATTERNS):
        return "ê¸ì •", SENTIMENT_WEIGHTS["positive"]
    if any(token in text for token in NEGATIVE_PATTERNS):
        return "ë¶€ì •", SENTIMENT_WEIGHTS["negative"]
    if any(token in text for token in NEUTRAL_PATTERNS):
        return "ì¤‘ë¦½", SENTIMENT_WEIGHTS["neutral"]
    return "ë¯¸í™•ì¸", 0


def safe_number(value: object) -> Optional[float]:
    if pd.isna(value):
        return None
    try:
        return float(value)
    except (TypeError, ValueError):
        return None


def parse_last_contact(row: pd.Series) -> Tuple[Optional[int], Optional[datetime]]:
    candidate_keys = ["ìµœê·¼ìƒë‹´ì¼", "ìµœì¢…ìƒë‹´ì¼", "ë§ˆì§€ë§‰ìƒë‹´ì¼", "ìƒë‹´ì¼"]
    for key in candidate_keys:
        if key in row:
            parsed = pd.to_datetime(row[key], errors="coerce")
            if pd.notna(parsed):
                days = (datetime.now().date() - parsed.date()).days
                return days, parsed.to_pydatetime()
    return None, None


def apply_rule_based_intelligence(df: pd.DataFrame) -> pd.DataFrame:
    enriched = df.copy()
    text_cache = {idx: combine_row_text(row) for idx, row in df.iterrows()}
    tag_list: List[str] = []
    score_list: List[int] = []
    sentiment_list: List[str] = []
    action_list: List[str] = []
    grade_list: List[str] = []
    keyword_hits: List[int] = []

    for idx, row in df.iterrows():
        combined_text = text_cache[idx]
        tags, keyword_score = keyword_features(combined_text)
        sentiment_label, sentiment_score = detect_sentiment(combined_text)

        score = keyword_score + sentiment_score

        consult_count = None
        for key in ["ìƒë‹´íšŸìˆ˜", "ìƒë‹´ íšŸìˆ˜", "ìƒë‹´íšŒìˆ˜"]:
            if key in row:
                consult_count = safe_number(row[key])
                break

        if consult_count is not None:
            if consult_count >= 4:
                score += 15
            elif consult_count >= 2:
                score += 8
            else:
                score += 2

        days_since_last, _ = parse_last_contact(row)
        if days_since_last is not None:
            if days_since_last <= 14:
                score += 15
            elif days_since_last <= 30:
                score += 5
            else:
                score -= 5

        score = max(0, min(100, score))

        if score >= HOT_THRESHOLD:
            grade = "Hot"
            action = "ì¦‰ì‹œ ì—°ë½í•˜ì—¬ ê³„ì•½ ì¶”ì§„ì„ í™•ì¸í•˜ì„¸ìš”."
        elif score >= WARM_THRESHOLD:
            grade = "Warm"
            action = "3~5ì¼ ë‚´ í›„ì† ìƒë‹´ì„ ì œì•ˆí•˜ì„¸ìš”."
        else:
            grade = "Cold"
            action = "ê´€ì‹¬ì„ ëŒë§Œí•œ ì •ë³´ë¥¼ ì œê³µí•˜ê³  ì¥ê¸° ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”."

        tag_list.append(", ".join(tags) if tags else "íƒœê·¸ ì—†ìŒ")
        score_list.append(score)
        sentiment_list.append(sentiment_label)
        action_list.append(action)
        grade_list.append(grade)

        keyword_hits.append(
            sum(
                1
                for rule in KEYWORD_RULES
                if any(keyword in combined_text for keyword in rule["keywords"])
            )
        )

    enriched["ì¶”ì²œíƒœê·¸"] = tag_list
    enriched["ë¦¬ë“œì ìˆ˜"] = score_list
    enriched["ê°ì„±ë¶„ì„"] = sentiment_list
    enriched["ì¶”ì²œì•¡ì…˜"] = action_list
    enriched["ë¦¬ë“œë“±ê¸‰"] = grade_list
    enriched["_keyword_hits"] = keyword_hits
    return enriched


def build_analysis_snapshot(df: pd.DataFrame, tokens: List[str]) -> Dict[str, object]:
    grade_counts = df["ë¦¬ë“œë“±ê¸‰"].value_counts().to_dict() if "ë¦¬ë“œë“±ê¸‰" in df.columns else {}
    sentiment_counts = (
        df["ê°ì„±ë¶„ì„"].value_counts().to_dict() if "ê°ì„±ë¶„ì„" in df.columns else {}
    )

    tag_counter: Counter[str] = Counter()
    if "ì¶”ì²œíƒœê·¸" in df.columns:
        for raw_tags in df["ì¶”ì²œíƒœê·¸"].dropna():
            for tag in str(raw_tags).split(","):
                cleaned = tag.strip()
                if cleaned and cleaned != "íƒœê·¸ ì—†ìŒ":
                    tag_counter[cleaned] += 1

    avg_score = float(df["ë¦¬ë“œì ìˆ˜"].mean()) if "ë¦¬ë“œì ìˆ˜" in df.columns else None

    return {
        "total_results": int(len(df)),
        "grade_counts": grade_counts,
        "sentiment_counts": sentiment_counts,
        "top_tags": tag_counter.most_common(10),
        "average_score": avg_score,
        "prompt_tokens": tokens,
    }


def read_excel(file_data: bytes) -> pd.DataFrame:
    with io.BytesIO(file_data) as buffer:
        df = pd.read_excel(buffer, engine="openpyxl")
    df.columns = df.columns.map(lambda col: str(col).strip())
    return df


def store_dataframe(df: pd.DataFrame) -> None:
    st.session_state["raw_dataframe"] = df
    st.session_state["raw_preview"] = df.head(10)


def set_prompt(prompt: str) -> None:
    st.session_state["search_prompt"] = prompt


def filter_dataframe(df: pd.DataFrame, prompt: str) -> pd.DataFrame:
    tokens = tokenize_prompt(prompt)
    if not tokens:
        capped = df.head(MAX_RESULTS)
        return apply_rule_based_intelligence(capped)

    lowercase_df = df.applymap(lambda value: str(value).lower())
    token_matches_list = []

    for token in tokens:
        token_lower = token.lower()
        token_matches = lowercase_df.apply(
            lambda col: col.str.contains(token_lower, na=False, regex=False)
        ).any(axis=1)
        token_matches_list.append(token_matches)

    if not token_matches_list:
        capped = df.head(MAX_RESULTS)
        return apply_rule_based_intelligence(capped)

    match_counts = sum(token_matches_list)
    required_matches = max(1, math.ceil(len(tokens) * 0.5))
    matched_rows = match_counts >= required_matches

    filtered = df.loc[matched_rows].copy()
    if filtered.empty:
        return filtered

    match_ratio = (match_counts[matched_rows] / len(tokens)).rename("_match_ratio")
    filtered = filtered.assign(_match_ratio=match_ratio, _match_score=match_counts[matched_rows])
    filtered = filtered.sort_values(by=["_match_ratio", "_match_score"], ascending=False)
    filtered = filtered.head(MAX_RESULTS)
    filtered = apply_rule_based_intelligence(filtered)
    return filtered.drop(columns=["_match_ratio", "_match_score", "_keyword_hits"], errors="ignore")


def update_filtered_results(df: pd.DataFrame, prompt: str) -> None:
    tokens = tokenize_prompt(prompt)
    filtered = filter_dataframe(df, prompt)
    st.session_state["filtered_dataframe"] = filtered
    st.session_state["analysis_snapshot"] = build_analysis_snapshot(filtered, tokens)
    st.session_state["ai_strategy"] = generate_ai_strategy(prompt, filtered, tokens)


def main() -> None:
    st.set_page_config(
        page_title="ê±°ë˜ì²˜ íƒìƒ‰ ë„ìš°ë¯¸",
        page_icon="ğŸ“‹",
        layout="wide",
    )

    st.title("ê±°ë˜ì²˜ íƒìƒ‰ ë„ìš°ë¯¸")
    st.write("300ê°œê°€ ë„˜ëŠ” ê±°ë˜ì²˜ë¥¼ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•´ ì í•©í•œ í›„ë³´ë¥¼ ë¹ ë¥´ê²Œ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ë„ì™€ë“œë¦½ë‹ˆë‹¤.")

    with st.sidebar:
        st.header("ê·œì¹™ ê¸°ë°˜ ì§€ëŠ¥ ìš”ì•½")
        st.markdown(
            "- í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜, ìƒë‹´ íšŸìˆ˜, ìµœê·¼ ìƒë‹´ì¼ì„ ì¡°í•©í•˜ì—¬ ë¦¬ë“œ ì ìˆ˜ë¥¼ ì‚°ì¶œí•©ë‹ˆë‹¤.\n"
            "- ê°ì„± íŒ¨í„´ì„ ê°ì§€í•´ ê¸ì •/ì¤‘ë¦½/ë¶€ì • ìƒí™©ì„ êµ¬ë¶„í•©ë‹ˆë‹¤.\n"
            "- ë¦¬ë“œ ë“±ê¸‰(Hot/Warm/Cold) ë³„ë¡œ í›„ì† ì•¡ì…˜ì„ ì¶”ì²œí•©ë‹ˆë‹¤."
        )
        rule_df = pd.DataFrame(KEYWORD_RULES)
        sidebar_df = rule_df.rename(columns={"tag": "íƒœê·¸", "keywords": "í‚¤ì›Œë“œ", "weight": "ê°€ì¤‘ì¹˜"})
        st.dataframe(sidebar_df, use_container_width=True, hide_index=True)
        st.caption("í•„ìš”ì— ë”°ë¼ í‚¤ì›Œë“œì™€ ê°€ì¤‘ì¹˜ë¥¼ ì½”ë“œì—ì„œ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    uploaded_file = st.file_uploader("ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ", type=["xlsx", "xls"])
    prompt = st.text_input("ê²€ìƒ‰ í”„ë¡¬í”„íŠ¸", placeholder="ì˜ˆ: ì„œìš¸ ì§€ì—­ IT ê´€ë ¨ ê±°ë˜ì²˜")

    col1, col2 = st.columns([1, 1])

    with col1:
        st.page_link("pages/1_results.py", label="ê²°ê³¼ í˜ì´ì§€ë¡œ ì´ë™", icon="â¡ï¸")

    with col2:
        if st.button("í”„ë¡¬í”„íŠ¸ì— ë§ëŠ” ê±°ë˜ì²˜ ì°¾ê¸°", use_container_width=True):
            if uploaded_file is None:
                st.warning("ë¨¼ì € ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            else:
                try:
                    dataframe = read_excel(uploaded_file.read())
                except ValueError as error:
                    st.error(f"ì—‘ì…€ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error}")
                    return

                store_dataframe(dataframe)
                set_prompt(prompt)
                update_filtered_results(dataframe, prompt)
                st.success("ê²°ê³¼ í˜ì´ì§€ì—ì„œ ê±°ë˜ì²˜ ëª©ë¡ì„ í™•ì¸í•˜ì„¸ìš”.")

    if "raw_dataframe" in st.session_state:
        st.subheader("ìµœê·¼ ì—…ë¡œë“œ ìš”ì•½")
        preview_df = st.session_state["raw_preview"]
        st.dataframe(preview_df, use_container_width=True)

    if "analysis_snapshot" in st.session_state:
        snapshot = st.session_state["analysis_snapshot"]
        if snapshot:
            st.subheader("ìµœê·¼ ê·œì¹™ ê¸°ë°˜ ë¶„ì„ í•˜ì´ë¼ì´íŠ¸")
            col_total, col_avg, col_hot = st.columns(3)
            col_total.metric("ì´ ê²°ê³¼", snapshot.get("total_results", 0))
            avg_score = snapshot.get("average_score")
            col_avg.metric(
                "í‰ê·  ë¦¬ë“œ ì ìˆ˜",
                f"{avg_score:.1f}" if avg_score is not None else "-",
            )
            col_hot.metric("Hot ë¦¬ë“œ", snapshot.get("grade_counts", {}).get("Hot", 0))

            top_tags = snapshot.get("top_tags", [])
            if top_tags:
                st.markdown(
                    "Â· ìƒìœ„ íƒœê·¸: "
                    + ", ".join(
                        f"{tag} ({count})" for tag, count in top_tags[:5]
                    )
                )

    st.markdown("---")
    st.markdown(
        """
        ### ì‚¬ìš© ê°€ì´ë“œ
        - `ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ`: ê±°ë˜ì²˜ ì •ë³´ê°€ ë‹´ê¸´ ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.
        - `ê²€ìƒ‰ í”„ë¡¬í”„íŠ¸`: ì°¾ê³  ì‹¶ì€ ê±°ë˜ì²˜ì˜ ì¡°ê±´ì„ ì…ë ¥í•˜ì„¸ìš”. ì—¬ëŸ¬ í‚¤ì›Œë“œëŠ” ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤.
        - ê·œì¹™ ê¸°ë°˜ ì—”ì§„ì´ ìë™ìœ¼ë¡œ ë¦¬ë“œ ë“±ê¸‰ê³¼ ì¶”ì²œ ì•¡ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.
        - `ê²°ê³¼ í˜ì´ì§€`: í•„í„°ë§ëœ ê±°ë˜ì²˜ ëª©ë¡ì„ í™•ì¸í•˜ê³  ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
    )


if __name__ == "__main__":
    main()

